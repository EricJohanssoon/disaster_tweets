{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proper imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ml imports\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# HF imports\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, set_seed\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "Shape of the dataframe: (7613, 5) \n",
      "\n",
      "0\n",
      "61\n",
      "2533\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# import train as df\n",
    "df = pd.read_csv('train.csv')\n",
    "print('Columns:',df.columns)\n",
    "print('Shape of the dataframe:', df.shape, '\\n')\n",
    "\n",
    "#checking out nan values in text columns\n",
    "print(df['id'].isnull().sum())\n",
    "print(df['keyword'].isnull().sum())\n",
    "print(df['location'].isnull().sum())\n",
    "print(df['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7578  10830  wrecked                            NaN   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "7582  10834  wrecked                            NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32    We always try to bring the heavy. #metal #RT h...       0  \n",
       "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "34                   Crying out for more! Set me ablaze       0  \n",
       "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "...                                                 ...     ...  \n",
       "7578   @jt_ruff23 @cameronhacker and I wrecked you both       0  \n",
       "7579  Three days off from work and they've pretty mu...       0  \n",
       "7580  #FX #forex #trading Cramer: Iger's 3 words tha...       0  \n",
       "7581  @engineshed Great atmosphere at the British Li...       0  \n",
       "7582  Cramer: Iger's 3 words that wrecked Disney's s...       0  \n",
       "\n",
       "[7552 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at rows where keyword is not nan\n",
    "df[df['keyword'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      " 0    3473\n",
      "1    2617\n",
      "Name: target, dtype: int64 \n",
      "\n",
      "val: \n",
      " 0    869\n",
      "1    654\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shuffle data\n",
    "df = shuffle(df,random_state=random_seed)\n",
    "\n",
    "# split data into train and val\n",
    "df_train, df_val = train_test_split(df, test_size=0.20, stratify=df['target']) \n",
    "\n",
    "# distributions of target\n",
    "print(\"train: \\n\",df_train['target'].value_counts(), \"\\n\")\n",
    "print(\"val: \\n\",df_val['target'].value_counts())\n",
    "\n",
    "#drop columns that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes to HuggingFace dataset dictionary\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "dataset = DatasetDict({\"train\": dataset_train, \"val\": dataset_val})\n",
    "\n",
    "# rename index and labels column\n",
    "dataset = dataset.rename_columns({'__index_level_0__': 'idx', 'target': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: distilroberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e0fa5bf20f46ac897ee2f0fea7081d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d378e70f5bdb4b5dbf7628baa9d15a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = 'distilroberta-base'\n",
    "\n",
    "# load pretrained tokenizer (checkpoint must match with model)\n",
    "print(f\"Checkpoint: {checkpoint}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "     # no padding, handled by data collator in dynamic fashion\n",
    "     return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# tokenize data\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# data collator to deal with dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210e378071de46a5835df01021e723bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducability\n",
    "set_seed(random_seed)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    num_labels=2, \n",
    "    problem_type='single_label_classification'\n",
    "    )\n",
    "\n",
    "# hyperparams \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"junk_filter_model_1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    #learning_rate=5e-6, # set to 5e-6 for roberta-large to avoid local optimum\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='precision',\n",
    "    seed=random_seed\n",
    "    )\n",
    "\n",
    "# adding more metrics to the trainer\n",
    "metrics = {}\n",
    "for metric in ['accuracy','precision', 'recall']:\n",
    "    metrics[metric] = load_metric(metric,'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric1 = load_metric(\"precision\")\n",
    "    metric2 = load_metric(\"recall\")\n",
    "    metric3 = load_metric(\"accuracy\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = metric1.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    accuracy = metric3.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# define a trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, id, text, keyword, location. If idx, id, text, keyword, location are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6090\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb15daec473646868fdaec4a0cf3d8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericjohansson/Documents/Github/Kaggle/nlp-getting-started/disaster_tweets.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericjohansson/Documents/Github/Kaggle/nlp-getting-started/disaster_tweets.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# launch trainer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericjohansson/Documents/Github/Kaggle/nlp-getting-started/disaster_tweets.ipynb#ch0000010?line=1'>2</a>\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1419'>1420</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1420'>1421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1421'>1422</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1423'>1424</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1424'>1425</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1425'>1426</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1426'>1427</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1427'>1428</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1428'>1429</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=1429'>1430</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=2026'>2027</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=2027'>2028</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=2028'>2029</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/transformers/trainer.py?line=2030'>2031</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/ericjohansson/opt/miniconda3/envs/junk-filter-env/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# launch trainer\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f828781220b021d9830e0a4e92cee8ee3446c374f72433392c3b0ac39ab736dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
